---
title: "P8105 - Homework 3"
author: "Tucker Morgan (tlm2152)"
date: "10/13/2021"
output: github_document
---

```{r setup, message = FALSE}
library(tidyverse)
library(viridis)
library(lubridate)
```

## Problem 1
I'll start by loading in the `instacart` dataset.
```{r instacart import}
library(p8105.datasets)
data("instacart")
head(instacart)
```
There are a total of `r nrow(instacart)` observations in the dataset and `r ncol(instacart)` variables. These data are arranged in a `tibble` and describe shopping information including `order_id`, `product_id`, `product name`, and `aisle`. For instance, for `order_id = 1`, the customer ordered Bulgarian Yogurt from the yoghurt aisle, Organic Celery Hearts from fresh vegetables, and Grated Pecorino Romano Cheese from the specialty cheeses section among other items.
```{r aisle summary}
count(distinct(instacart, aisle_id))
instacart %>% 
  group_by(aisle) %>% 
  summarize(n_purch = n()) %>% 
  arrange(desc(n_purch))
```
There are a total of `r count(distinct(instacart, aisle_id))` unique aisles. The aisles ordered from most frequently are `fresh vegetables`, `fresh fruits`, and `packaged vegetables fruits`. This is somewhat comforting.
```{r aisle plot}
instacart %>% 
  group_by(aisle) %>% 
  summarize(n_purch = n()) %>% 
  filter(n_purch >= 10000) %>% 
  arrange(n_purch) %>% 
  mutate(aisle = factor(aisle, levels = aisle[order(n_purch, decreasing = TRUE)])) %>% 
  ggplot(aes(x = aisle, y = n_purch, fill = aisle)) +
  geom_bar(stat = "identity", alpha = 0.9) +
  theme(legend.position = "none",
        axis.text.x = element_text(angle = -90, vjust = .35, hjust = 0)) +
  scale_fill_viridis_d() +
  labs(x = "Aisle Name", y = "Number of Purchases", title = "Number of Instacart Purchases by Aisle") +
  theme(plot.title = element_text(face = "bold"))
```

Next, I'll look at the most ordered items in the "baking ingredients", "dog food care", and "packaged vegetables fruits" aisles.
```{r aisle table}
aisle_table <- 
  instacart %>% 
  filter(aisle == c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>% 
  select(aisle, product_name) %>% 
  group_by(aisle, product_name) %>% 
  summarize(n_purch = n()) %>% 
  arrange(desc(n_purch)) %>% 
  slice(1:3) %>% 
  mutate(rank = rank(desc(n_purch))) %>% 
  mutate(product_count = paste(product_name, "-", n_purch, "orders")) %>% 
  select(-n_purch, -product_name) %>% 
  pivot_wider(names_from = aisle, values_from = product_count)
knitr::kable(aisle_table,
             format = "simple",
             caption = "**Top Three Purchased Products by Aisle**",
             col.names = str_to_title(names(aisle_table)))
```

I'll also look at the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week.
```{r apples and ice cream, message = FALSE}
daily_table <- 
  instacart %>% 
  filter(product_name == c("Coffee Ice Cream", "Pink Lady Apples")) %>% 
  select(order_dow, order_hour_of_day, product_name) %>% 
  group_by(product_name, order_dow) %>% 
  summarize(daily_mean = round(mean(order_hour_of_day), digits = 2)) %>% 
  pivot_wider(names_from = product_name, values_from = daily_mean) %>% 
  mutate(order_dow = wday(order_dow + 1, label = TRUE))
knitr::kable(daily_table,
             format = "simple",
             caption = "**Mean Order Time on Each Day of the Week**",
             col.names = c("Day", "Coffee Ice Cream", "Pink Lady Apples"))
```

Looks like Pink Lady Apples are typically purchased earlier in the day, around noon, whereas Coffee Ice Cream might be more of an afternoon treat. It would be interesting to test for a significant difference between these times.

## Problem 2
I'll start by loading in the `brfss_smart2010` dataset and cleaning it up a bit.
```{r BRFSS import}
data("brfss_smart2010")
head(brfss_smart2010)
```

```{r BRFSS cleaning}
brfss_clean <- 
  brfss_smart2010 %>% 
  janitor::clean_names() %>% 
  filter(topic == "Overall Health") %>% 
  mutate(response = as_factor(response)) %>% 
  arrange(desc(response)) %>% 
  rename(state = locationabbr, location_desc = locationdesc, resp_id = respid)
```

```{r observations by state}
brfss_states <- 
  brfss_clean %>% 
  group_by(year, state) %>% 
  summarize(state_count = n()) %>% 
  filter(year == 2002 | year == 2010, state_count >= 7) %>% 
  pivot_wider(names_from = year, values_from = state_count)
knitr::kable(brfss_states,
             format = "simple",
             caption = "**States with 7 or more observation sites in 2002 and 2010**",
             col.names = c("State", "2002", "2010"))
```

There are `r nrow(brfss_states %>% select("2002") %>% drop_na())` states with 7 or more observation locations in 2002 compared to `r nrow(brfss_states %>% select("2010") %>% drop_na())` states with 7 or more observation locations in 2010.

I'll look at a spaghetti plot of the `mean_data_value` for each `state`.
```{r spaghetti plot}
brfss_clean %>% 
  filter(response == "Excellent") %>% 
  group_by(state, year) %>% 
  summarize(mean_data_value = mean(data_value)) %>% 
  ggplot(aes(x = year, y = mean_data_value, group = state, color = state)) +
  geom_line() +
  scale_color_viridis_d() +
  labs(x = "Year", y = "Mean Data Value", title = "Mean Data Value by State") +
  theme(plot.title = element_text(face = "bold"))
```

```{r New York State}
brfss_clean %>% 
  filter(year == "2006" | year == "2010", state == "NY") %>% 
  ggplot(aes(x = response, y = data_value, color = year)) +
  geom_point() +
  facet_grid(~year) +
  theme(legend.position = "none") +
  labs(x = "Response", y = "Data Value", title = "Data Value Distributions in New York State") +
  theme(plot.title = element_text(face = "bold"))
```

## Problem 3

```{r accelerometer data import, message = FALSE}
accel_data_df <- 
  read_csv("./Data/accel_data.csv") %>% 
  mutate(day = factor(day,
                      levels = c("Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday",  "Saturday"))) %>% 
  pivot_longer(cols = activity.1:activity.1440,
               names_to = "minute",
               names_prefix = "activity.",
               values_to = "activity_level") %>% 
  mutate(minute = as.double(minute)) %>% 
  mutate(week_end = as.factor(case_when(
    day == "Saturday" ~ "weekend",
    day == "Sunday" ~ "weekend",
    TRUE ~ "weekday"
    )))
head(accel_data_df)
```
I've read in the `accel_data.csv` file and changed some of the variables. I converted `day` to a factor variable with seven levels, one for each day. I pivoted the `activity.1:activity.1440` variables into one `minute` variable with the values going to `activity_level`. So instead of 1,443 variables, I have `r ncol(accel_data_df)` variables with `r nrow(accel_data_df)` observations. I've added a `week_end` factor variable to indicate if the day is during the week (Monday - Friday) or the weekend (Saturday and Sunday).

```{r accelerometer daily table}
accel_daily_table <- 
  accel_data_df %>% 
  group_by(day) %>% 
  summarize(total_activity = sum(activity_level))
knitr::kable(accel_daily_table,
             format = "simple",
             caption = "**Total Activity Each Day of the Week**",
             col.names = c("Day", "Total Activity"))
```

Saturday is by far the day with the least activity with Wednesday, Thursday, and Friday having the most activity.

```{r accelerometer daily plot}
accel_plot <- 
  accel_data_df %>% 
  group_by(day) %>% 
  ggplot(aes(x = minute, y = activity_level, group = day, color = day)) +
  geom_line(alpha = 0.7) + 
  geom_smooth(se = FALSE, color = "black", size = 1.5) +
  geom_smooth(se = FALSE, size = 1) +
  scale_color_viridis_d(name = "Day of the Week") +
  labs(x = "Time of Day", y = "Activity Level", title = "Daily Activity for Each Day of the Week") +
  scale_x_continuous(breaks = c(0, 360, 720, 1080, 1440),
                     labels = c("Midnight", "6:00am", "Noon", "6:00pm", "Midnight")) +
  theme(plot.title = element_text(face = "bold"))
accel_plot
```

Based on the plot above, I can make a few general observations:
  
  * The time of day with the lowest typical activity level across all seven days of the week is from midnight to 6:00am. And we can see that the activity level starts to taper off at the end of the day approaching midnight. This is not surprising since this is the time of day that most people sleep and are inactive.
  * Activity seems to peak around noon, particularly on Sunday, and in the evening, particularly on Friday evening. This could be a result of social gatherings for the participant.
  * Although Saturday is the least active day overall, Saturday evening has higher activity levels than Sunday and Tuesday. I've added a zoomed-in version of the plot below so the smoothed lines can be seen more distinctly.

```{r zoomed-in accel plot}
accel_plot + coord_cartesian(ylim = c(0, 2000)) + theme(legend.position = "bottom")
```

