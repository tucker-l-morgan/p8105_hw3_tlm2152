P8105 - Homework 3
================
Tucker Morgan (tlm2152)
10/13/2021

``` r
library(tidyverse)
library(viridis)
library(lubridate)
```

## Problem 1

I’ll start by loading in the `instacart` dataset.

``` r
library(p8105.datasets)
data("instacart")
head(instacart)
```

    ## # A tibble: 6 × 15
    ##   order_id product_id add_to_cart_order reordered user_id eval_set order_number
    ##      <int>      <int>             <int>     <int>   <int> <chr>           <int>
    ## 1        1      49302                 1         1  112108 train               4
    ## 2        1      11109                 2         1  112108 train               4
    ## 3        1      10246                 3         0  112108 train               4
    ## 4        1      49683                 4         0  112108 train               4
    ## 5        1      43633                 5         1  112108 train               4
    ## 6        1      13176                 6         0  112108 train               4
    ## # … with 8 more variables: order_dow <int>, order_hour_of_day <int>,
    ## #   days_since_prior_order <int>, product_name <chr>, aisle_id <int>,
    ## #   department_id <int>, aisle <chr>, department <chr>

There are a total of 1384617 observations in the dataset and 15
variables. These data are arranged in a `tibble` and describe shopping
information including `order_id`, `product_id`, `product name`, and
`aisle`. For instance, for `order_id = 1`, the customer ordered
Bulgarian Yogurt from the yoghurt aisle, Organic Celery Hearts from
fresh vegetables, and Grated Pecorino Romano Cheese from the specialty
cheeses section among other items.

``` r
count(distinct(instacart, aisle_id))
```

    ## # A tibble: 1 × 1
    ##       n
    ##   <int>
    ## 1   134

``` r
instacart %>% 
  group_by(aisle) %>% 
  summarize(n_purch = n()) %>% 
  arrange(desc(n_purch))
```

    ## # A tibble: 134 × 2
    ##    aisle                         n_purch
    ##    <chr>                           <int>
    ##  1 fresh vegetables               150609
    ##  2 fresh fruits                   150473
    ##  3 packaged vegetables fruits      78493
    ##  4 yogurt                          55240
    ##  5 packaged cheese                 41699
    ##  6 water seltzer sparkling water   36617
    ##  7 milk                            32644
    ##  8 chips pretzels                  31269
    ##  9 soy lactosefree                 26240
    ## 10 bread                           23635
    ## # … with 124 more rows

There are a total of 134 unique aisles. The aisles ordered from most
frequently are `fresh vegetables`, `fresh fruits`, and
`packaged vegetables fruits`. This is somewhat comforting.

``` r
instacart %>% 
  group_by(aisle) %>% 
  summarize(n_purch = n()) %>% 
  filter(n_purch >= 10000) %>% 
  arrange(n_purch) %>% 
  mutate(aisle = factor(aisle, levels = aisle[order(n_purch, decreasing = TRUE)])) %>% 
  ggplot(aes(x = aisle, y = n_purch, fill = aisle)) +
  geom_bar(stat = "identity", alpha = 0.9) +
  theme(legend.position = "none",
        axis.text.x = element_text(angle = -90, vjust = .35, hjust = 0)) +
  scale_fill_viridis_d() +
  labs(x = "Aisle Name", y = "Number of Purchases", title = "Figure 1.1: Number of Instacart Purchases by Aisle") +
  theme(plot.title = element_text(face = "bold"))
```

![](p8105_hw3_tlm2152_files/figure-gfm/aisle%20plot-1.png)<!-- -->

Next, I’ll look at the most ordered items in the “baking ingredients”,
“dog food care”, and “packaged vegetables fruits” aisles.

``` r
aisle_table <- 
  instacart %>% 
  filter(aisle == c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>% 
  select(aisle, product_name) %>% 
  group_by(aisle, product_name) %>% 
  summarize(n_purch = n()) %>% 
  arrange(desc(n_purch)) %>% 
  slice(1:3) %>% 
  mutate(rank = rank(desc(n_purch))) %>% 
  mutate(product_count = paste(product_name, "-", n_purch, "orders")) %>% 
  select(-n_purch, -product_name) %>% 
  pivot_wider(names_from = aisle, values_from = product_count)
```

    ## `summarise()` has grouped output by 'aisle'. You can override using the `.groups` argument.

``` r
knitr::kable(aisle_table,
             format = "pipe",
             caption = "**Table 1.1: Top Three Purchased Products by Aisle**",
             col.names = str_to_title(names(aisle_table)))
```

| Rank | Baking Ingredients                   | Dog Food Care                                               | Packaged Vegetables Fruits         |
|-----:|:-------------------------------------|:------------------------------------------------------------|:-----------------------------------|
|    1 | Light Brown Sugar - 157 orders       | Organix Grain Free Chicken & Vegetable Dog Food - 14 orders | Organic Baby Spinach - 3324 orders |
|    2 | Pure Baking Soda - 140 orders        | Organix Chicken & Brown Rice Recipe - 13 orders             | Organic Raspberries - 1920 orders  |
|    3 | Organic Vanilla Extract - 122 orders | Original Dry Dog - 9 orders                                 | Organic Blueberries - 1692 orders  |

**Table 1.1: Top Three Purchased Products by Aisle**

I’ll also look at the mean hour of the day at which Pink Lady Apples and
Coffee Ice Cream are ordered on each day of the week.

``` r
daily_table <- 
  instacart %>% 
  filter(product_name == c("Coffee Ice Cream", "Pink Lady Apples")) %>% 
  select(order_dow, order_hour_of_day, product_name) %>% 
  group_by(product_name, order_dow) %>% 
  summarize(daily_mean = round(mean(order_hour_of_day), digits = 2)) %>% 
  pivot_wider(names_from = product_name, values_from = daily_mean) %>% 
  mutate(order_dow = wday(order_dow + 1, label = TRUE))
```

    ## Warning in product_name == c("Coffee Ice Cream", "Pink Lady Apples"): longer
    ## object length is not a multiple of shorter object length

``` r
knitr::kable(daily_table,
             format = "simple",
             caption = "**Table 1.2: Mean Order Time on Each Day of the Week**",
             col.names = c("Day", "Coffee Ice Cream", "Pink Lady Apples"))
```

| Day | Coffee Ice Cream | Pink Lady Apples |
|:----|-----------------:|-----------------:|
| Sun |            14.54 |            14.50 |
| Mon |            13.14 |            10.95 |
| Tue |            15.42 |            11.36 |
| Wed |            15.25 |            14.56 |
| Thu |            15.27 |            11.33 |
| Fri |            14.00 |            11.89 |
| Sat |            15.16 |            12.43 |

**Table 1.2: Mean Order Time on Each Day of the Week**

Looks like Pink Lady Apples are typically purchased earlier in the day,
around noon, whereas Coffee Ice Cream might be more of an afternoon
treat. It would be interesting to test for a significant difference
between these times.

## Problem 2

I’ll start by loading in the `brfss_smart2010` dataset and cleaning it
up a bit.

``` r
data("brfss_smart2010")
head(brfss_smart2010)
```

    ## # A tibble: 6 × 23
    ##    Year Locationabbr Locationdesc  Class  Topic  Question   Response Sample_Size
    ##   <int> <chr>        <chr>         <chr>  <chr>  <chr>      <chr>          <int>
    ## 1  2010 AL           AL - Jeffers… Healt… Overa… How is yo… Excelle…          94
    ## 2  2010 AL           AL - Jeffers… Healt… Overa… How is yo… Very go…         148
    ## 3  2010 AL           AL - Jeffers… Healt… Overa… How is yo… Good             208
    ## 4  2010 AL           AL - Jeffers… Healt… Overa… How is yo… Fair             107
    ## 5  2010 AL           AL - Jeffers… Healt… Overa… How is yo… Poor              45
    ## 6  2010 AL           AL - Jeffers… Healt… Fair … Health St… Good or…         450
    ## # … with 15 more variables: Data_value <dbl>, Confidence_limit_Low <dbl>,
    ## #   Confidence_limit_High <dbl>, Display_order <int>, Data_value_unit <chr>,
    ## #   Data_value_type <chr>, Data_Value_Footnote_Symbol <chr>,
    ## #   Data_Value_Footnote <chr>, DataSource <chr>, ClassId <chr>, TopicId <chr>,
    ## #   LocationID <chr>, QuestionID <chr>, RESPID <chr>, GeoLocation <chr>

``` r
brfss_clean <- 
  brfss_smart2010 %>% 
  janitor::clean_names() %>% 
  filter(topic == "Overall Health") %>% 
  mutate(response = as_factor(response)) %>% 
  arrange(desc(response)) %>% 
  rename(state = locationabbr, location_desc = locationdesc, resp_id = respid)
rm(brfss_smart2010)
```

``` r
brfss_states <- 
  brfss_clean %>% 
  group_by(year, state) %>% 
  summarize(state_count = n()) %>% 
  filter(year == 2002 | year == 2010, state_count >= 7) %>% 
  pivot_wider(names_from = year, values_from = state_count)
```

    ## `summarise()` has grouped output by 'year'. You can override using the `.groups` argument.

``` r
knitr::kable(brfss_states,
             format = "simple",
             caption = "**Table 2.1: States with 7 or more observation sites in 2002 and 2010**",
             col.names = c("State", "2002", "2010"))
```

| State | 2002 | 2010 |
|:------|-----:|-----:|
| AZ    |   10 |   15 |
| CO    |   20 |   35 |
| CT    |   35 |   25 |
| DE    |   15 |   15 |
| FL    |   35 |  205 |
| GA    |   15 |   20 |
| HI    |   20 |   20 |
| ID    |   10 |   30 |
| IL    |   15 |   10 |
| IN    |   10 |   15 |
| KS    |   15 |   20 |
| LA    |   15 |   25 |
| MA    |   40 |   45 |
| MD    |   30 |   60 |
| ME    |   10 |   30 |
| MI    |   20 |   20 |
| MN    |   20 |   25 |
| MO    |   10 |   15 |
| NC    |   35 |   60 |
| NE    |   15 |   50 |
| NH    |   25 |   25 |
| NJ    |   40 |   95 |
| NV    |   10 |   10 |
| NY    |   25 |   45 |
| OH    |   20 |   40 |
| OK    |   15 |   15 |
| OR    |   15 |   20 |
| PA    |   50 |   35 |
| RI    |   20 |   25 |
| SC    |   15 |   35 |
| SD    |   10 |   10 |
| TN    |   10 |   25 |
| TX    |   10 |   80 |
| UT    |   25 |   30 |
| VT    |   15 |   30 |
| WA    |   20 |   50 |
| AL    |   NA |   15 |
| AR    |   NA |   15 |
| CA    |   NA |   60 |
| IA    |   NA |   10 |
| MS    |   NA |   10 |
| MT    |   NA |   15 |
| ND    |   NA |   15 |
| NM    |   NA |   30 |
| WY    |   NA |   10 |

**Table 2.1: States with 7 or more observation sites in 2002 and 2010**

There are 36 states with 7 or more observation locations in 2002
compared to 45 states with 7 or more observation locations in 2010. It
looks like several more rural states were added between 2002 and 2010 -
Alabama, Iowa, Montana, Wyoming, etc.

I’ll look at a spaghetti plot of the `mean_data_value` for each `state`.
First, I’ll create a reference data frame with every state and region
per the US Census Bureau.

``` r
NE <- tibble(state = c("CT","ME","MA","NH","RI","VT","NJ","NY","PA"),
             region = c("northeast"))

MW <- tibble(state = c("IN","IL","MI","OH","WI","IA","KS","MN","MO","NE","ND","SD"),
             region = "midwest")

S <- tibble(state = c("DE","DC","FL","GA","MD","NC","SC","VA","WV","AL","KY","MS","TN","AR","LA","OK","TX"),
            region = "south")

W <- tibble(state = c("AZ","CO","ID","NM","MT","UT","NV","WY","AK","CA","HI","OR","WA"),
            region = "west")

region_df <- bind_rows(NE, MW, S, W)
rm(NE, MW, S, W)
```

``` r
brfss_clean %>% 
  filter(response == "Excellent") %>% 
  group_by(state, year) %>% 
  summarize(mean_data_value = mean(data_value)) %>% 
  left_join(y = region_df, by = "state") %>% 
  ggplot(aes(x = year, y = mean_data_value, group = state, color = region)) +
  geom_line() +
  scale_color_viridis_d() +
  labs(x = "Year", y = "Mean Data Value", title = "Figure 2.1: Mean Data Value by State, Colored by Region") +
  theme(plot.title = element_text(face = "bold"))
```

    ## `summarise()` has grouped output by 'state'. You can override using the `.groups` argument.

    ## Warning: Removed 3 row(s) containing missing values (geom_path).

![](p8105_hw3_tlm2152_files/figure-gfm/spaghetti%20plot-1.png)<!-- -->

I’ve used US Census regions to group the various states into geographic
regions. Looking at the data by region, it seems like the `south` in
green has the most variability with some of the highest values and some
of the lowest.

``` r
brfss_clean %>% 
  filter(year == "2006" | year == "2010", state == "NY") %>% 
  ggplot(aes(x = response, y = data_value, color = year)) +
  geom_point() +
  facet_grid(~year) +
  theme(legend.position = "none") +
  labs(x = "Response", y = "Data Value", title = "Figure 2.2: Data Value Distributions in New York State") +
  theme(plot.title = element_text(face = "bold"))
```

![](p8105_hw3_tlm2152_files/figure-gfm/New%20York%20State-1.png)<!-- -->

It looks like the 2010 values have a bit wider spread compared to 2006,
particularly in the “Fair” and “Very Good” categories.

## Problem 3

``` r
accel_data_df <- 
  read_csv("./Data/accel_data.csv") %>% 
  mutate(day = factor(day,
                      levels = c("Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday",  "Saturday"))) %>% 
  pivot_longer(cols = activity.1:activity.1440,
               names_to = "minute",
               names_prefix = "activity.",
               values_to = "activity_level") %>% 
  mutate(minute = as.double(minute)) %>% 
  mutate(week_end = as.factor(case_when(
    day == "Saturday" ~ "weekend",
    day == "Sunday" ~ "weekend",
    TRUE ~ "weekday"
    )))
head(accel_data_df)
```

    ## # A tibble: 6 × 6
    ##    week day_id day    minute activity_level week_end
    ##   <dbl>  <dbl> <fct>   <dbl>          <dbl> <fct>   
    ## 1     1      1 Friday      1           88.4 weekday 
    ## 2     1      1 Friday      2           82.2 weekday 
    ## 3     1      1 Friday      3           64.4 weekday 
    ## 4     1      1 Friday      4           70.0 weekday 
    ## 5     1      1 Friday      5           75.0 weekday 
    ## 6     1      1 Friday      6           66.3 weekday

I’ve read in the `accel_data.csv` file and changed some of the
variables. I converted `day` to a factor variable with seven levels, one
for each day. I pivoted the `activity.1:activity.1440` variables into
one `minute` variable with the values going to `activity_level`. So
instead of 1,443 variables, I have 6 variables with 50400 observations.
I’ve added a `week_end` factor variable to indicate if the day is during
the week (Monday - Friday) or the weekend (Saturday and Sunday).

``` r
accel_daily_table <- 
  accel_data_df %>% 
  group_by(day) %>% 
  summarize(total_activity = sum(activity_level))
knitr::kable(accel_daily_table,
             format = "simple",
             caption = "**Table 3.1: Total Activity Each Day of the Week**",
             col.names = c("Day", "Total Activity"))
```

| Day       | Total Activity |
|:----------|---------------:|
| Sunday    |        1919213 |
| Monday    |        1858699 |
| Tuesday   |        1799238 |
| Wednesday |        2129772 |
| Thursday  |        2091151 |
| Friday    |        2291711 |
| Saturday  |        1369237 |

**Table 3.1: Total Activity Each Day of the Week**

Saturday is by far the day with the least activity with Wednesday,
Thursday, and Friday having the most activity.

``` r
accel_plot <- 
  accel_data_df %>% 
  group_by(day) %>% 
  ggplot(aes(x = minute, y = activity_level, group = day, color = day)) +
  geom_line(alpha = 0.7) + 
  geom_smooth(se = FALSE, color = "black", size = 1.5) +
  geom_smooth(se = FALSE, size = 1) +
  scale_color_viridis_d(name = "Day of the Week") +
  labs(x = "Time of Day", y = "Activity Level", title = "Figure 3.1: Daily Activity for Each Day of the Week") +
  scale_x_continuous(breaks = c(0, 360, 720, 1080, 1440),
                     labels = c("Midnight", "6:00am", "Noon", "6:00pm", "Midnight")) +
  theme(plot.title = element_text(face = "bold"))
accel_plot
```

    ## `geom_smooth()` using method = 'gam' and formula 'y ~ s(x, bs = "cs")'
    ## `geom_smooth()` using method = 'gam' and formula 'y ~ s(x, bs = "cs")'

![](p8105_hw3_tlm2152_files/figure-gfm/accelerometer%20daily%20plot-1.png)<!-- -->

Based on the plot above, I can make a few general observations:

-   The time of day with the lowest typical activity level across all
    seven days of the week is from midnight to 6:00am. And we can see
    that the activity level starts to taper off at the end of the day
    approaching midnight. This is not surprising since this is the time
    of day that most people sleep and are inactive.
-   Activity seems to peak around noon, particularly on Sunday, and in
    the evening, particularly on Friday evening. This could be a result
    of social gatherings for the participant.
-   Although Saturday is the least active day overall, Saturday evening
    has higher activity levels than Sunday and Tuesday. I’ve added a
    zoomed-in version of the plot below so the smoothed lines can be
    seen more distinctly.

``` r
accel_plot + coord_cartesian(ylim = c(0, 2000)) + theme(legend.position = "bottom")
```

    ## `geom_smooth()` using method = 'gam' and formula 'y ~ s(x, bs = "cs")'
    ## `geom_smooth()` using method = 'gam' and formula 'y ~ s(x, bs = "cs")'

![](p8105_hw3_tlm2152_files/figure-gfm/zoomed-in%20accel%20plot-1.png)<!-- -->
