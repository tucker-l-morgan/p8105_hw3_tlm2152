P8105 - Homework 3
================
Tucker Morgan (tlm2152)
10/13/2021

``` r
library(tidyverse)
library(viridis)
library(lubridate)
```

## Problem 1

I’ll start by loading in the `instacart` dataset.

``` r
library(p8105.datasets)
data("instacart")
head(instacart)
```

    ## # A tibble: 6 × 15
    ##   order_id product_id add_to_cart_order reordered user_id eval_set order_number
    ##      <int>      <int>             <int>     <int>   <int> <chr>           <int>
    ## 1        1      49302                 1         1  112108 train               4
    ## 2        1      11109                 2         1  112108 train               4
    ## 3        1      10246                 3         0  112108 train               4
    ## 4        1      49683                 4         0  112108 train               4
    ## 5        1      43633                 5         1  112108 train               4
    ## 6        1      13176                 6         0  112108 train               4
    ## # … with 8 more variables: order_dow <int>, order_hour_of_day <int>,
    ## #   days_since_prior_order <int>, product_name <chr>, aisle_id <int>,
    ## #   department_id <int>, aisle <chr>, department <chr>

There are a total of 1384617 observations in the dataset and 15
variables. These data are arranged in a `tibble` and describe shopping
information including `order_id`, `product_id`, `product name`, and
`aisle`. For instance, for `order_id = 1`, the customer ordered
Bulgarian Yogurt from the yoghurt aisle, Organic Celery Hearts from
fresh vegetables, and Grated Pecorino Romano Cheese from the specialty
cheeses section among other items.

``` r
count(distinct(instacart, aisle_id))
```

    ## # A tibble: 1 × 1
    ##       n
    ##   <int>
    ## 1   134

``` r
instacart %>% 
  group_by(aisle) %>% 
  summarize(n_purch = n()) %>% 
  arrange(desc(n_purch))
```

    ## # A tibble: 134 × 2
    ##    aisle                         n_purch
    ##    <chr>                           <int>
    ##  1 fresh vegetables               150609
    ##  2 fresh fruits                   150473
    ##  3 packaged vegetables fruits      78493
    ##  4 yogurt                          55240
    ##  5 packaged cheese                 41699
    ##  6 water seltzer sparkling water   36617
    ##  7 milk                            32644
    ##  8 chips pretzels                  31269
    ##  9 soy lactosefree                 26240
    ## 10 bread                           23635
    ## # … with 124 more rows

There are a total of 134 unique aisles. The aisles ordered from most
frequently are `fresh vegetables`, `fresh fruits`, and
`packaged vegetables fruits`. This is somewhat comforting.

``` r
instacart %>% 
  group_by(aisle) %>% 
  summarize(n_purch = n()) %>% 
  filter(n_purch >= 10000) %>% 
  arrange(n_purch) %>% 
  mutate(aisle = factor(aisle, levels = aisle[order(n_purch, decreasing = TRUE)])) %>% 
  ggplot(aes(x = aisle, y = n_purch, fill = aisle)) +
  geom_bar(stat = "identity", alpha = 0.9) +
  theme(legend.position = "none",
        axis.text.x = element_text(angle = -90, vjust = .35, hjust = 0)) +
  scale_fill_viridis_d() +
  labs(x = "Aisle Name", y = "Number of Purchases", title = "Number of Instacart Purchases by Aisle")
```

![](p8105_hw3_tlm2152_files/figure-gfm/aisle%20plot-1.png)<!-- -->

Next, I’ll look at the most ordered items in the “baking ingredients”,
“dog food care”, and “packaged vegetables fruits” aisles.

``` r
aisle_table <- 
  instacart %>% 
  filter(aisle == c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>% 
  select(aisle, product_name) %>% 
  group_by(aisle, product_name) %>% 
  summarize(n_purch = n()) %>% 
  arrange(desc(n_purch)) %>% 
  slice(1:3) %>% 
  mutate(rank = rank(desc(n_purch))) %>% 
  mutate(product_count = paste(product_name, "-", n_purch, "orders")) %>% 
  select(-n_purch, -product_name) %>% 
  pivot_wider(names_from = aisle, values_from = product_count)
```

    ## `summarise()` has grouped output by 'aisle'. You can override using the `.groups` argument.

``` r
knitr::kable(aisle_table,
             format = "simple",
             caption = "Top Three Purchased Products by Aisle")
```

| rank | baking ingredients                   | dog food care                                               | packaged vegetables fruits         |
|-----:|:-------------------------------------|:------------------------------------------------------------|:-----------------------------------|
|    1 | Light Brown Sugar - 157 orders       | Organix Grain Free Chicken & Vegetable Dog Food - 14 orders | Organic Baby Spinach - 3324 orders |
|    2 | Pure Baking Soda - 140 orders        | Organix Chicken & Brown Rice Recipe - 13 orders             | Organic Raspberries - 1920 orders  |
|    3 | Organic Vanilla Extract - 122 orders | Original Dry Dog - 9 orders                                 | Organic Blueberries - 1692 orders  |

Top Three Purchased Products by Aisle

Next I’ll look at the mean hour of the day at which Pink Lady Apples and
Coffee Ice Cream are ordered on each day of the week.

``` r
daily_table <- 
  instacart %>% 
  filter(product_name == c("Coffee Ice Cream", "Pink Lady Apples")) %>% 
  select(order_dow, order_hour_of_day, product_name) %>% 
  group_by(product_name, order_dow) %>% 
  summarize(daily_mean = round(mean(order_hour_of_day), digits = 2)) %>% 
  pivot_wider(names_from = product_name, values_from = daily_mean) %>% 
  mutate(order_dow = wday(order_dow + 1, label = TRUE))
```

    ## Warning in product_name == c("Coffee Ice Cream", "Pink Lady Apples"): longer
    ## object length is not a multiple of shorter object length

    ## `summarise()` has grouped output by 'product_name'. You can override using the `.groups` argument.

``` r
knitr::kable(daily_table,
             format = "simple",
             caption = "Mean Order Time on Each Day of the Week")
```

| order\_dow | Coffee Ice Cream | Pink Lady Apples |
|:-----------|-----------------:|-----------------:|
| Sun        |            14.54 |            14.50 |
| Mon        |            13.14 |            10.95 |
| Tue        |            15.42 |            11.36 |
| Wed        |            15.25 |            14.56 |
| Thu        |            15.27 |            11.33 |
| Fri        |            14.00 |            11.89 |
| Sat        |            15.16 |            12.43 |

Mean Order Time on Each Day of the Week

Looks like Pink Lady Apples are typically purchased earlier in the day,
around noon, whereas Coffee Ice Cream might be more of an afternoon
treat. It would be interesting to test for a significant difference
between these times.

## Problem 2

I’ll start by loading in the `brfss_smart2010` dataset and cleaning it
up a bit.

``` r
data("brfss_smart2010")
head(brfss_smart2010)
```

    ## # A tibble: 6 × 23
    ##    Year Locationabbr Locationdesc  Class  Topic  Question   Response Sample_Size
    ##   <int> <chr>        <chr>         <chr>  <chr>  <chr>      <chr>          <int>
    ## 1  2010 AL           AL - Jeffers… Healt… Overa… How is yo… Excelle…          94
    ## 2  2010 AL           AL - Jeffers… Healt… Overa… How is yo… Very go…         148
    ## 3  2010 AL           AL - Jeffers… Healt… Overa… How is yo… Good             208
    ## 4  2010 AL           AL - Jeffers… Healt… Overa… How is yo… Fair             107
    ## 5  2010 AL           AL - Jeffers… Healt… Overa… How is yo… Poor              45
    ## 6  2010 AL           AL - Jeffers… Healt… Fair … Health St… Good or…         450
    ## # … with 15 more variables: Data_value <dbl>, Confidence_limit_Low <dbl>,
    ## #   Confidence_limit_High <dbl>, Display_order <int>, Data_value_unit <chr>,
    ## #   Data_value_type <chr>, Data_Value_Footnote_Symbol <chr>,
    ## #   Data_Value_Footnote <chr>, DataSource <chr>, ClassId <chr>, TopicId <chr>,
    ## #   LocationID <chr>, QuestionID <chr>, RESPID <chr>, GeoLocation <chr>

``` r
brfss_clean <- 
  brfss_smart2010 %>% 
  janitor::clean_names() %>% 
  filter(topic == "Overall Health") %>% 
  mutate(response = as_factor(response)) %>% 
  arrange(desc(response)) %>% 
  rename(location_abbr = locationabbr, location_desc = locationdesc, resp_id = respid)
```

``` r
brfss_states <- 
  brfss_clean %>% 
  group_by(year, location_abbr) %>% 
  summarize(state_count = n()) %>% 
  filter(year == 2002 | year == 2010, state_count >= 7) %>% 
  pivot_wider(names_from = year, values_from = state_count)
## `summarise()` has grouped output by 'year'. You can override using the `.groups` argument.
knitr::kable(brfss_states,
             format = "simple",
             caption = "States with 7 or more observation sites in 2002 and 2010")
```

| location\_abbr | 2002 | 2010 |
|:---------------|-----:|-----:|
| AZ             |   10 |   15 |
| CO             |   20 |   35 |
| CT             |   35 |   25 |
| DE             |   15 |   15 |
| FL             |   35 |  205 |
| GA             |   15 |   20 |
| HI             |   20 |   20 |
| ID             |   10 |   30 |
| IL             |   15 |   10 |
| IN             |   10 |   15 |
| KS             |   15 |   20 |
| LA             |   15 |   25 |
| MA             |   40 |   45 |
| MD             |   30 |   60 |
| ME             |   10 |   30 |
| MI             |   20 |   20 |
| MN             |   20 |   25 |
| MO             |   10 |   15 |
| NC             |   35 |   60 |
| NE             |   15 |   50 |
| NH             |   25 |   25 |
| NJ             |   40 |   95 |
| NV             |   10 |   10 |
| NY             |   25 |   45 |
| OH             |   20 |   40 |
| OK             |   15 |   15 |
| OR             |   15 |   20 |
| PA             |   50 |   35 |
| RI             |   20 |   25 |
| SC             |   15 |   35 |
| SD             |   10 |   10 |
| TN             |   10 |   25 |
| TX             |   10 |   80 |
| UT             |   25 |   30 |
| VT             |   15 |   30 |
| WA             |   20 |   50 |
| AL             |   NA |   15 |
| AR             |   NA |   15 |
| CA             |   NA |   60 |
| IA             |   NA |   10 |
| MS             |   NA |   10 |
| MT             |   NA |   15 |
| ND             |   NA |   15 |
| NM             |   NA |   30 |
| WY             |   NA |   10 |

States with 7 or more observation sites in 2002 and 2010

There are 36 states with 7 or more observation locations in 2002
compared to 45 states with 7 or more observation locations in 2010.
